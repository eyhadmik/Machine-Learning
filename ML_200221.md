## k-최근접 이웃(k-Nearest Neighbor, kNN)
- 장점
    1. 다른 머신러닝 알고리즘보다 이해하기가 쉽다. 다른 알고리즘은 미적분, 확률 및 정보 이론 등의 기본 지식이 필요한 데 비해 kNN 알고리즘은 수학적으로 거리를 계산하는 방법만 알면 이해하기가 쉽다.
    2. 숫자로 구분된 속성에 우수한 성능을 보인다. 거리, 횟수, 점수와 같이 수치화된 데이터에 대해서는 거리 기반 머신러닝 알고리즘인 kNN 알고리즘을 사용하면 높은 정확도를 기대할 수 있다.
    3. 별도의 모델 학습이 필요 없다. kNN 알고리즘은 예측을 하는 시점에서 모든 기존 데이터와의 거리를 계산하기 때문에 예측 전에 모델을 따로 학습시킬 필요가 없다.(Lazy learning)
- 단점
    1. 예측 속도가 느리다. 하나의 데이터를 예측할 때마다 전체 데이터와의 거리를 계산 하기 때문에 연산 속도가 다른 알고리즘에 비해 느리다. 비교할 속성이 많아질수록 연산 속도는 더 느려진다.
    2. 다른 머신러닝 알고리즘에 비해 예측값이 지역 정보에 많이 편향될 수 있다. 다른 머신러닝 알고리즘은 예측값이 기존의 전체 데이터에서 학습된 모델에서 나오는 반면 kNN 알고리즘은 오직 가까운 이웃을 통해 예측하기 때문이다. 아무리 좋은 데이터를 가지고 있더라도 kNN 알고리즘에서는 k의 개수가 적거나 몇 개의 예외적인 데이터가 이웃으로 존재할 경우 예측값이 틀릴 가능성이 높아진다.

## 서포트 벡터 머신(Support Vector Machine, SVM)
### 서포트 벡터 : 결정 경계를 만드는 데 영향을 주는 최전방 데이터 포인트
### 결정 경계 : 서로 다른 분류값을 결정하는 경계
- 데이터의 벡터 공간을 N차원이라고 할 경우, 결정 경계 = N - 1 차원
### 마진 : 결정 경계와 서포트 벡터 사이의 거리
### SVM의 목표 : 마진을 최대로 하는 결정 경계를 찾는 것. 마진이 클수록 현재 알지 못하는 새로운 데이터에 대해 안정적으로 분류할 가능성이 높다.
### SVM에서는 약간의 오류를 허용하기 위해 비용이라는 변수를 사용한다.
- 비용이 낮을수록, 마진을 최대한 높이고, 학습 에러율을 증가시키는 방향으로 결정 경계선을 만든다.
- 비용이 높을수록, 마진은 낮아지고, 학습 에러율은 감소하는 방향으로 결정 경계선을 만든다.
- 비용이 너무 낮으면 과소적합의 위험이 있고, 너무 높으면 과대적합의 위험이 있으니 적절한 비용 값을 찾는 과정이 중요하다.
### 커널 트릭
- SVM의 핵심은 주어진 데이터 분포 속에서 결정 경계를 찾아내는 것
- 2차원 공간에서는 두 개의 데이터 집합을 가로지르는 선을 찾는 것이고, 3차원 공간에서는 두 개의 데이터 집합을 가로지르는 평면을 찾는 것이다.
- 1차원의 결정 경계는 0차원으로 나타나기 때문에 점 하나로 집단을 구분해야하고, 구분 방법은 존재하지 않는다.
- N-1차원의 평면으로 두 데이터 집단을 구분해야 하는 SVM의 입장에서 이 문제는 상당히 흥미로운 문제였으며, 주어진 저차원 벡터 공간의 데이터(벡터)를 고차원 벡터 공간으로 옮겨줌으로써 결정 경계를 찾는 방법을 고안하게 되었다.
- 매핑 함수 : 저차원의 데이터를 고차원의 데이터로 옮겨주는 함수
    - 매핑 함수를 가지고 실제로 많은 양의 데이터를 저차원에서 고차원으로 옮기기에는 계산량이 너무 많아서 현실적으로 사용하기가 어렵다.
    - 그래서 실제로 데이터를 고차원으로 보내진 않지만 보낸 것과 동일한 효과를 줘서 매우 빠른 속도로 결정 경계선을 찾는 방법이 바로 **커널 트릭** 이다.
- SVM이 2차원 벡터 공간 상에서 직선이 아닌 결정 경계선으로 데이터를 분류한 예제는 모두 커널 트릭의 결과이다.
- 선형 SVM : 커널을 사용하지 않고 데이터를 분류. 비용을 조절해서 마진의 크기를 조절할 수 있다.
- 선형 분리가 주어진 차원에서 불가능할 경우 고차원으로 데이터를 옮기는 효과를 통해 결정 경계를 찾는다. 비용과 감마를 조절해서 마진을 조절할 수 있다.
- 가우시안 RBF 커널 : 일반적으로 많이 사용되는 커널 기법의 종류. 데이터 포인트에 적용되는 가우시안 함수의 표준편차를 조정함으로써 결정 경계의 곡률을 조정한다. 이 표준편차 조정 변수를 감마라고 부른다.
### 파라미터 튜닝
- 정확도를 높이기 위해 조절 가능한 파라미터 : 비용(cost), 감마(gamma)
- 비용(cost) : 마진 너비 조절 변수. 클수록 마진 너비가 좁아지고, 작을수록 마진 너비가 넓어진다.
- 감마(gamma) : 커널의 표준 편차 조절 변수. 작을수록 데이터 포인트의 영향이 커져서 경계가 완만해지고, 클수록 데이터 포인트가 결정 경계에 영향을 적게 미쳐 경계가 구부러진다.
### SVM 알고리즘의 장점과 단점
- 장점
    1. 커널 트릭을 사용함으로써 특성이 다양한 데이터를 분류하는 데 강하다.
    2. 파라미터(cost, gamma)를 조정해서 과대적합 및 과소적합에 대처할 수 있다.
    3. 적은 학습 데이터로도 딥러닝만큼 정확도가 높은 분류를 기대할 수 있다.
- 단점
    1. 데이터 전처리 과정이 중요하다.특성이 비슷한 수치로 구성된 데이터는 SVM을 쉽게 활용할 수 있지만 특성이 다양하거니 확연히 다른 경우에는 데이터 전처리 과정을 통해 데이터 특성 그대로 벡터 공간에 표현해야 한다.
    2. 특성이 많을 경우 결정 경계 및 데이터의 시각화가 어렵다.